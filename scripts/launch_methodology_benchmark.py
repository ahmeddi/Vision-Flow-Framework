#!/usr/bin/env python3
"""
Script de lancement rapide pour le benchmark m√©thodologique complet.
Permet de lancer facilement des exp√©riences avec diff√©rents profils de test.
"""

import argparse
import subprocess
import sys
from pathlib import Path

class MethodologyLauncher:
    """Lanceur simplifi√© pour les benchmarks m√©thodologiques."""
    
    def __init__(self):
        self.available_models = [
            'yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt',
            'yolov11n.pt', 'yolov11s.pt', 'yolov11m.pt', 'yolov11l.pt', 'yolov11x.pt',
            'yolov7.pt'
        ]
        
        self.available_datasets = ['weed25', 'deepweeds', 'cwd30', 'weedsgalore', 'dummy']
        
        self.profiles = {
            'quick': {
                'description': 'Test rapide (5 √©poques, YOLOv8n seulement)',
                'epochs': 5,
                'models': ['yolov8n.pt'],
                'datasets': ['dummy'],
                'batch_size': 16
            },
            'development': {
                'description': 'D√©veloppement (10 √©poques, mod√®les nano/small)',
                'epochs': 10,
                'models': ['yolov8n.pt', 'yolov8s.pt', 'yolov11n.pt'],
                'datasets': ['weed25'],
                'batch_size': 8
            },
            'validation': {
                'description': 'Validation (20 √©poques, s√©lection de mod√®les)',
                'epochs': 20,
                'models': ['yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', 'yolov11n.pt', 'yolov11s.pt'],
                'datasets': ['weed25', 'deepweeds'],
                'batch_size': 8
            },
            'publication': {
                'description': 'Publication (50 √©poques, tous mod√®les disponibles)',
                'epochs': 50,
                'models': None,  # Tous les mod√®les disponibles
                'datasets': None,  # Tous les datasets disponibles
                'batch_size': 8
            },
            'full': {
                'description': 'Benchmark complet (100 √©poques, tous mod√®les, tous datasets)',
                'epochs': 100,
                'models': None,
                'datasets': None,
                'batch_size': 4  # Batch size r√©duit pour les gros mod√®les
            }
        }
    
    def show_profiles(self):
        """Affiche les profils disponibles."""
        print("üìã PROFILS DE BENCHMARK DISPONIBLES:")
        print("=" * 60)
        
        for name, config in self.profiles.items():
            print(f"\nüîß {name.upper()}")
            print(f"   {config['description']}")
            print(f"   √âpoques: {config['epochs']}")
            print(f"   Mod√®les: {len(config['models']) if config['models'] else 'tous'}")
            print(f"   Datasets: {len(config['datasets']) if config['datasets'] else 'tous'}")
            print(f"   Batch size: {config['batch_size']}")
    
    def estimate_duration(self, profile_name: str) -> str:
        """Estime la dur√©e du benchmark."""
        config = self.profiles[profile_name]
        
        num_models = len(config['models']) if config['models'] else len(self.available_models)
        num_datasets = len(config['datasets']) if config['datasets'] else len(self.available_datasets)
        epochs = config['epochs']
        
        # Estimation grossi√®re: ~2min/√©poque pour YOLOv8n, plus pour les autres
        base_time_per_epoch = 2  # minutes
        model_factor = 1.5  # Facteur moyen pour les mod√®les plus gros
        
        estimated_minutes = num_models * num_datasets * epochs * base_time_per_epoch * model_factor
        
        if estimated_minutes < 60:
            return f"~{estimated_minutes:.0f} minutes"
        elif estimated_minutes < 1440:  # 24h
            return f"~{estimated_minutes/60:.1f} heures"
        else:
            return f"~{estimated_minutes/1440:.1f} jours"
    
    def launch_benchmark(self, profile_name: str, custom_args: dict = None) -> bool:
        """Lance un benchmark avec le profil sp√©cifi√©."""
        
        if profile_name not in self.profiles:
            print(f"‚ùå Profil '{profile_name}' non reconnu")
            return False
        
        config = self.profiles[profile_name]
        
        # Construire la commande
        cmd = [sys.executable, 'scripts/comprehensive_methodology_benchmark.py']
        
        # Ajouter les arguments du profil
        cmd.extend(['--epochs', str(config['epochs'])])
        cmd.extend(['--batch_size', str(config['batch_size'])])
        
        if config['models']:
            cmd.extend(['--models'] + config['models'])
        
        if config['datasets']:
            cmd.extend(['--datasets'] + config['datasets'])
        
        # Ajouter les arguments personnalis√©s
        if custom_args:
            for key, value in custom_args.items():
                if value is not None:
                    cmd.extend([f'--{key}', str(value)])
        
        # Estimation de dur√©e
        duration = self.estimate_duration(profile_name)
        
        print(f"\nüöÄ LANCEMENT DU BENCHMARK - PROFIL {profile_name.upper()}")
        print("=" * 60)
        print(f"üìä {config['description']}")
        print(f"‚è±Ô∏è  Dur√©e estim√©e: {duration}")
        print(f"üíª Commande: {' '.join(cmd)}")
        
        # Demander confirmation pour les longs benchmarks
        if profile_name in ['publication', 'full']:
            response = input(f"\n‚ö†Ô∏è  Ce benchmark prendra {duration}. Continuer? (y/N): ")
            if response.lower() != 'y':
                print("‚ùå Benchmark annul√©")
                return False
        
        print(f"\nüîÑ D√©marrage du benchmark...")
        
        try:
            # Lancer le benchmark
            result = subprocess.run(cmd, cwd=Path.cwd())
            
            if result.returncode == 0:
                print(f"\n‚úÖ Benchmark {profile_name} termin√© avec succ√®s!")
                
                # G√©n√©rer automatiquement les analyses
                print("üìä G√©n√©ration des analyses...")
                self.generate_analysis()
                
                return True
            else:
                print(f"\n‚ùå Benchmark {profile_name} √©chou√© (code: {result.returncode})")
                return False
                
        except KeyboardInterrupt:
            print(f"\n‚èπÔ∏è  Benchmark {profile_name} interrompu par l'utilisateur")
            return False
        except Exception as e:
            print(f"\nüí• Erreur lors du benchmark: {e}")
            return False
    
    def generate_analysis(self):
        """G√©n√®re automatiquement les analyses et la documentation."""
        
        try:
            # Analyser les r√©sultats
            print("üîç Analyse des r√©sultats...")
            subprocess.run([sys.executable, 'scripts/analyze_methodology_results.py'], 
                         check=True, cwd=Path.cwd())
            
            # G√©n√©rer la documentation
            print("üìö G√©n√©ration de la documentation...")
            subprocess.run([sys.executable, 'scripts/generate_methodology_documentation.py'], 
                         check=True, cwd=Path.cwd())
            
            print("‚úÖ Analyses et documentation g√©n√©r√©es!")
            
        except subprocess.CalledProcessError as e:
            print(f"‚ö†Ô∏è  Erreur lors de la g√©n√©ration d'analyses: {e}")
        except Exception as e:
            print(f"üí• Erreur inattendue: {e}")
    
    def run_custom_benchmark(self, args):
        """Lance un benchmark avec des param√®tres personnalis√©s."""
        
        cmd = [sys.executable, 'scripts/comprehensive_methodology_benchmark.py']
        
        # Ajouter tous les arguments personnalis√©s
        if args.epochs:
            cmd.extend(['--epochs', str(args.epochs)])
        if args.batch_size:
            cmd.extend(['--batch_size', str(args.batch_size)])
        if args.models:
            cmd.extend(['--models'] + args.models)
        if args.datasets:
            cmd.extend(['--datasets'] + args.datasets)
        if args.device:
            cmd.extend(['--device', args.device])
        if args.no_augmentation:
            cmd.append('--no-augmentation')
        if args.no_validation:
            cmd.append('--no-validation')
        
        print(f"\nüöÄ BENCHMARK PERSONNALIS√â")
        print("=" * 40)
        print(f"üíª Commande: {' '.join(cmd)}")
        
        try:
            result = subprocess.run(cmd, cwd=Path.cwd())
            
            if result.returncode == 0:
                print("\n‚úÖ Benchmark personnalis√© termin√©!")
                self.generate_analysis()
                return True
            else:
                print(f"\n‚ùå Benchmark √©chou√© (code: {result.returncode})")
                return False
                
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Benchmark interrompu")
            return False

def main():
    parser = argparse.ArgumentParser(description='Lanceur de benchmark m√©thodologique')
    parser.add_argument('--profile', choices=['quick', 'development', 'validation', 'publication', 'full'],
                       help='Profil de benchmark pr√©d√©fini')
    parser.add_argument('--list-profiles', action='store_true',
                       help='Afficher les profils disponibles')
    
    # Arguments pour benchmark personnalis√©
    parser.add_argument('--epochs', type=int, help='Nombre d\'√©poques')
    parser.add_argument('--batch_size', type=int, help='Taille du batch')
    parser.add_argument('--models', nargs='*', help='Mod√®les √† tester')
    parser.add_argument('--datasets', nargs='*', help='Datasets √† tester')
    parser.add_argument('--device', help='Device (auto/cpu/cuda)')
    parser.add_argument('--no-augmentation', action='store_true',
                       help='D√©sactiver l\'augmentation de donn√©es')
    parser.add_argument('--no-validation', action='store_true',
                       help='D√©sactiver la validation des r√©sultats')
    
    # Actions sp√©ciales
    parser.add_argument('--analyze-only', action='store_true',
                       help='Analyser les r√©sultats existants uniquement')
    parser.add_argument('--docs-only', action='store_true',
                       help='G√©n√©rer la documentation uniquement')
    
    args = parser.parse_args()
    
    launcher = MethodologyLauncher()
    
    # Afficher les profils
    if args.list_profiles:
        launcher.show_profiles()
        return
    
    # Actions sp√©ciales
    if args.analyze_only:
        print("üîç Analyse des r√©sultats existants...")
        launcher.generate_analysis()
        return
    
    if args.docs_only:
        print("üìö G√©n√©ration de la documentation...")
        subprocess.run([sys.executable, 'scripts/generate_methodology_documentation.py'])
        return
    
    # Benchmark avec profil
    if args.profile:
        launcher.launch_benchmark(args.profile)
        return
    
    # Benchmark personnalis√© si des arguments sont fournis
    if any([args.epochs, args.batch_size, args.models, args.datasets]):
        launcher.run_custom_benchmark(args)
        return
    
    # Sinon, mode interactif
    print("üéØ LANCEUR DE BENCHMARK M√âTHODOLOGIQUE")
    print("=" * 50)
    print("\nOptions disponibles:")
    print("1. Utiliser un profil pr√©d√©fini (--profile PROFIL)")
    print("2. Lancer un benchmark personnalis√© (--epochs X --models Y...)")
    print("3. Voir les profils disponibles (--list-profiles)")
    print("4. Analyser les r√©sultats existants (--analyze-only)")
    print("5. G√©n√©rer la documentation (--docs-only)")
    print("\nUtilisez --help pour voir toutes les options.")

if __name__ == '__main__':
    main()