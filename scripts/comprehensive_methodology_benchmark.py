#!/usr/bin/env python3
"""
Script de benchmark complet et mÃ©thodologique pour la comparaison de modÃ¨les de dÃ©tection d'objets.
ImplÃ©mente une mÃ©thodologie rigoureuse avec prÃ©traitement uniforme, entraÃ®nement standardisÃ©,
et mÃ©triques complÃ¨tes pour tous les modÃ¨les.

ModÃ¨les testÃ©s:
- YOLOv8 (n, s, m, l, x)
- YOLOv11 (n, s, m, l, x) 
- YOLO-NAS (s, m, l) - si disponible
- YOLOX (nano, tiny, s, m, l, x) - si disponible
- YOLOv7 (standard)
- EfficientDet (d0-d3)
- DETR (base)
- RT-DETR (l, x)

Datasets:
- Weed25 (25 espÃ¨ces de mauvaises herbes)
- DeepWeeds (8 espÃ¨ces, environnements variÃ©s)
- CWD30 (20 espÃ¨ces + cultures)
- WeedsGalore (UAV multispectral, segmentation)

MÃ©triques collectÃ©es:
- mAP@0.5, mAP@0.5:0.95
- FPS, latence
- Taille du modÃ¨le (paramÃ¨tres, MB)
- Consommation Ã©nergÃ©tique
- Temps d'entraÃ®nement
"""

import argparse
import json
import time
import subprocess
import sys
import psutil
import threading
import warnings
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import torch
import numpy as np
from contextlib import contextmanager

# Supprimer les warnings
warnings.filterwarnings('ignore')

class EnergyMonitor:
    """Moniteur de consommation Ã©nergÃ©tique."""
    
    def __init__(self):
        self.power_readings = []
        self.monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self):
        """DÃ©marre le monitoring Ã©nergÃ©tique."""
        self.monitoring = True
        self.power_readings = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
        
    def stop_monitoring(self) -> float:
        """ArrÃªte le monitoring et retourne la consommation moyenne."""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        
        if self.power_readings:
            return np.mean(self.power_readings)
        return 0.0
        
    def _monitor_loop(self):
        """Boucle de monitoring Ã©nergÃ©tique."""
        while self.monitoring:
            try:
                # Approximation via l'utilisation CPU
                cpu_percent = psutil.cpu_percent(interval=1)
                # Estimation grossiÃ¨re: 50W base + CPU% * 150W
                estimated_power = 50 + (cpu_percent / 100) * 150
                self.power_readings.append(estimated_power)
            except:
                pass
            time.sleep(1)

class UnifiedPreprocessor:
    """SystÃ¨me uniforme de prÃ©traitement pour tous les modÃ¨les."""
    
    def __init__(self, img_size: int = 640, augmentation: bool = True):
        self.img_size = img_size
        self.augmentation = augmentation
        
    def get_preprocessing_config(self) -> Dict[str, Any]:
        """Configuration de prÃ©traitement standard."""
        config = {
            'imgsz': self.img_size,
            'cache': False,  # Ã‰viter les problÃ¨mes de mÃ©moire
            'rect': False,   # Pas de rectangular training pour la consistance
            'mosaic': 1.0 if self.augmentation else 0.0,
            'mixup': 0.1 if self.augmentation else 0.0,
            'copy_paste': 0.1 if self.augmentation else 0.0,
            'degrees': 10.0 if self.augmentation else 0.0,
            'translate': 0.1 if self.augmentation else 0.0,
            'scale': 0.5 if self.augmentation else 0.0,
            'shear': 2.0 if self.augmentation else 0.0,
            'perspective': 0.0001 if self.augmentation else 0.0,
            'flipud': 0.5 if self.augmentation else 0.0,
            'fliplr': 0.5 if self.augmentation else 0.0,
            'hsv_h': 0.015 if self.augmentation else 0.0,
            'hsv_s': 0.7 if self.augmentation else 0.0,
            'hsv_v': 0.4 if self.augmentation else 0.0,
        }
        return config

class ModelFactory:
    """Factory pour crÃ©er et gÃ©rer tous les types de modÃ¨les."""
    
    def __init__(self):
        self.available_models = self._check_available_models()
        
    def _check_available_models(self) -> Dict[str, List[str]]:
        """VÃ©rifie quels modÃ¨les sont disponibles."""
        available = {
            'yolov8': ['yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt'],
            'yolov11': ['yolov11n.pt', 'yolov11s.pt', 'yolov11m.pt', 'yolov11l.pt', 'yolov11x.pt'],
            'yolov7': ['yolov7.pt'],
        }
        
        # VÃ©rifier YOLO-NAS
        try:
            import super_gradients
            available['yolo_nas'] = ['yolo_nas_s', 'yolo_nas_m', 'yolo_nas_l']
            print("âœ… YOLO-NAS disponible")
        except ImportError:
            print("âš ï¸  YOLO-NAS non disponible")
            
        # VÃ©rifier YOLOX
        try:
            import yolox
            available['yolox'] = ['yolox_nano', 'yolox_tiny', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x']
            print("âœ… YOLOX disponible")
        except ImportError:
            print("âš ï¸  YOLOX non disponible")
            
        # ModÃ¨les Transformers (DETR, EfficientDet)
        try:
            import timm
            import transformers
            available['detr'] = ['detr_resnet50']
            available['rt_detr'] = ['rtdetr_l', 'rtdetr_x']
            available['efficientdet'] = ['efficientdet_d0', 'efficientdet_d1', 'efficientdet_d2', 'efficientdet_d3']
            print("âœ… DETR/EfficientDet disponibles")
        except ImportError:
            print("âš ï¸  DETR/EfficientDet non disponibles")
            
        return available
        
    def get_all_models(self) -> List[str]:
        """Retourne la liste de tous les modÃ¨les disponibles."""
        all_models = []
        for model_family, models in self.available_models.items():
            all_models.extend(models)
        return all_models
        
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """Retourne les informations sur un modÃ¨le."""
        for family, models in self.available_models.items():
            if model_name in models:
                return {
                    'family': family,
                    'name': model_name,
                    'framework': self._get_framework(family)
                }
        return {'family': 'unknown', 'name': model_name, 'framework': 'unknown'}
        
    def _get_framework(self, family: str) -> str:
        """Retourne le framework utilisÃ© par une famille de modÃ¨les."""
        if family in ['yolov8', 'yolov11', 'yolov7']:
            return 'ultralytics'
        elif family == 'yolo_nas':
            return 'super_gradients'
        elif family == 'yolox':
            return 'yolox'
        elif family in ['detr', 'rt_detr', 'efficientdet']:
            return 'transformers'
        return 'unknown'

class MetricsCalculator:
    """Calculateur de mÃ©triques unifiÃ©es."""
    
    def __init__(self):
        self.energy_monitor = EnergyMonitor()
        
    @contextmanager
    def measure_energy(self):
        """Context manager pour mesurer l'Ã©nergie."""
        self.energy_monitor.start_monitoring()
        try:
            yield
        finally:
            energy_consumption = self.energy_monitor.stop_monitoring()
            
    def calculate_model_size(self, weights_path: str) -> Tuple[int, float]:
        """Calcule la taille du modÃ¨le en paramÃ¨tres et MB."""
        try:
            if weights_path and Path(weights_path).exists():
                # Taille du fichier en MB
                file_size_mb = Path(weights_path).stat().st_size / (1024 * 1024)
                
                # Essayer de charger le modÃ¨le pour compter les paramÃ¨tres
                try:
                    if weights_path.endswith('.pt'):
                        checkpoint = torch.load(weights_path, map_location='cpu')
                        if 'model' in checkpoint:
                            model = checkpoint['model']
                            if hasattr(model, 'parameters'):
                                total_params = sum(p.numel() for p in model.parameters())
                            else:
                                total_params = 0
                        else:
                            total_params = 0
                    else:
                        total_params = 0
                except:
                    total_params = 0
                    
                return total_params, file_size_mb
            else:
                return 0, 0.0
        except Exception as e:
            print(f"Erreur calcul taille modÃ¨le: {e}")
            return 0, 0.0
            
    def calculate_fps(self, model_path: str, dataset_yaml: str, device: str = 'cpu') -> float:
        """Calcule les FPS du modÃ¨le."""
        try:
            # Commande pour benchmark FPS
            cmd = [
                sys.executable, '-c', f'''
import torch
import time
from ultralytics import YOLO
try:
    model = YOLO("{model_path}")
    model.to("{device}")
    
    # DonnÃ©es factices pour le benchmark
    dummy_input = torch.randn(1, 3, 640, 640).to("{device}")
    
    # Warm-up
    for _ in range(10):
        with torch.no_grad():
            _ = model.model(dummy_input)
    
    # Mesure FPS
    start_time = time.time()
    num_runs = 100
    for _ in range(num_runs):
        with torch.no_grad():
            _ = model.model(dummy_input)
    
    total_time = time.time() - start_time
    fps = num_runs / total_time
    print(f"FPS: {{fps:.2f}}")
except Exception as e:
    print(f"FPS: 0.0")
'''
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                # Extraire FPS de la sortie
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if line.startswith('FPS:'):
                        return float(line.split(':')[1].strip())
            
            return 0.0
            
        except Exception as e:
            print(f"Erreur calcul FPS: {e}")
            return 0.0

class ComprehensiveMethodologyBenchmark:
    """Gestionnaire de benchmark mÃ©thodologique complet."""
    
    def __init__(self, 
                 device: str = 'auto',
                 epochs: int = 100,
                 batch_size: int = 16,
                 img_size: int = 640,
                 augmentation: bool = True,
                 patience: int = 50):
        
        self.device = device
        self.epochs = epochs
        self.batch_size = batch_size
        self.img_size = img_size
        self.patience = patience
        
        # Initialiser les composants
        self.preprocessor = UnifiedPreprocessor(img_size, augmentation)
        self.model_factory = ModelFactory()
        self.metrics_calculator = MetricsCalculator()
        
        # RÃ©pertoires
        self.results_dir = Path('results/comprehensive_methodology')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Datasets disponibles
        self.datasets = self._check_datasets()
        
    def _check_datasets(self) -> Dict[str, str]:
        """VÃ©rifie les datasets disponibles."""
        datasets = {
            'weed25': 'data/weed25.yaml',
            'deepweeds': 'data/deepweeds.yaml', 
            'cwd30': 'data/cwd30.yaml',
            'weedsgalore': 'data/weedsgalore.yaml',
            'dummy': 'data/dummy.yaml'  # Toujours inclure dummy pour les tests
        }
        
        available = {}
        for name, yaml_path in datasets.items():
            if Path(yaml_path).exists():
                data_dir = Path(f'data/{name}')
                if data_dir.exists() and any(data_dir.iterdir()):
                    available[name] = yaml_path
                    print(f"âœ… Dataset {name} disponible")
                else:
                    print(f"âš ï¸  Dataset {name}: YAML trouvÃ© mais donnÃ©es manquantes")
            else:
                print(f"âš ï¸  Dataset {name}: Configuration YAML manquante")
        
        # S'assurer qu'au moins dummy est disponible
        if 'dummy' not in available and Path('data/dummy.yaml').exists():
            available['dummy'] = 'data/dummy.yaml'
            print("âœ… Dataset dummy disponible (fallback)")
            
        if not available:
            print("âŒ Aucun dataset disponible!")
            
        return available
        
    def train_model_with_unified_protocol(self, 
                                        model_name: str, 
                                        dataset_name: str, 
                                        dataset_yaml: str) -> Dict[str, Any]:
        """EntraÃ®ne un modÃ¨le avec le protocole unifiÃ©."""
        
        print(f"\nğŸš€ EntraÃ®nement unifiÃ©: {model_name} sur {dataset_name}")
        
        # Configuration de prÃ©traitement
        preprocess_config = self.preprocessor.get_preprocessing_config()
        model_info = self.model_factory.get_model_info(model_name)
        
        # RÃ©pertoire de sortie
        output_dir = self.results_dir / f"{model_name}_{dataset_name}"
        output_dir.mkdir(exist_ok=True)
        
        # Commande d'entraÃ®nement avec paramÃ¨tres unifiÃ©s
        cmd = self._build_training_command(
            model_name, dataset_yaml, output_dir, 
            model_info, preprocess_config
        )
        
        # Monitoring Ã©nergÃ©tique
        energy_start = time.time()
        with self.metrics_calculator.measure_energy():
            try:
                start_time = time.time()
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2h timeout
                training_time = time.time() - start_time
                
                if result.returncode == 0:
                    print(f"âœ… EntraÃ®nement rÃ©ussi en {training_time:.1f}s")
                    
                    # Trouver le meilleur modÃ¨le
                    best_weights = self._find_best_weights(output_dir, model_name)
                    
                    return {
                        'status': 'success',
                        'training_time': training_time,
                        'weights_path': str(best_weights) if best_weights else None,
                        'output_dir': str(output_dir),
                        'model_info': model_info,
                        'preprocessing_config': preprocess_config
                    }
                else:
                    print(f"âŒ Ã‰chec entraÃ®nement: {result.stderr[:500]}")
                    return {
                        'status': 'failed',
                        'error': result.stderr[:500],
                        'training_time': training_time,
                        'model_info': model_info
                    }
                    
            except subprocess.TimeoutExpired:
                print(f"â° Timeout entraÃ®nement aprÃ¨s 2h")
                return {
                    'status': 'timeout',
                    'training_time': 7200,
                    'model_info': model_info
                }
            except Exception as e:
                print(f"ğŸ’¥ Erreur entraÃ®nement: {e}")
                return {
                    'status': 'error',
                    'error': str(e),
                    'training_time': 0,
                    'model_info': model_info
                }
                
    def _build_training_command(self, 
                              model_name: str, 
                              dataset_yaml: str, 
                              output_dir: Path,
                              model_info: Dict[str, Any], 
                              preprocess_config: Dict[str, Any]) -> List[str]:
        """Construit la commande d'entraÃ®nement selon le framework."""
        
        framework = model_info['framework']
        
        if framework == 'ultralytics':
            # Utiliser le script train.py existant
            cmd = [
                sys.executable, 'scripts/train.py',
                '--models', model_name,
                '--data', dataset_yaml,
                '--epochs', str(self.epochs),
                '--batch-size', str(self.batch_size),
                '--device', self.device,
                '--output', str(output_dir)
            ]
            
        else:
            # Fallback pour les autres frameworks
            print(f"âš ï¸  Framework {framework} non implÃ©mentÃ©, utilisation d'Ultralytics par dÃ©faut")
            cmd = [
                sys.executable, 'scripts/train.py',
                '--models', model_name,
                '--data', dataset_yaml,
                '--epochs', str(self.epochs),
                '--batch-size', str(self.batch_size),
                '--device', self.device,
                '--output', str(output_dir)
            ]
            
        return cmd
        
    def _find_best_weights(self, output_dir: Path, model_name: str) -> Optional[Path]:
        """Trouve le fichier de poids du meilleur modÃ¨le."""
        possible_paths = [
            output_dir / 'weights' / 'best.pt',
            output_dir / 'best.pt',
            output_dir / f'{model_name}' / 'weights' / 'best.pt',
            output_dir / f'{model_name}' / 'best.pt'
        ]
        
        for path in possible_paths:
            if path.exists():
                return path
                
        # Chercher rÃ©cursivement
        for path in output_dir.rglob('best.pt'):
            return path
            
        return None
        
    def evaluate_model_comprehensive(self, 
                                   weights_path: str, 
                                   dataset_yaml: str, 
                                   model_name: str, 
                                   dataset_name: str) -> Dict[str, Any]:
        """Ã‰valuation complÃ¨te avec toutes les mÃ©triques."""
        
        print(f"ğŸ“Š Ã‰valuation complÃ¨te: {model_name} sur {dataset_name}")
        
        # Ã‰valuation standard
        cmd = [
            sys.executable, 'scripts/evaluate.py',
            '--weights', weights_path,
            '--data', dataset_yaml,
            '--device', self.device,
            '--output', str(self.results_dir / f"eval_{model_name}_{dataset_name}.json")
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)
            
            eval_results = {}
            if result.returncode == 0:
                print(f"âœ… Ã‰valuation standard rÃ©ussie")
                
                # Charger les rÃ©sultats d'Ã©valuation
                eval_file = self.results_dir / f"eval_{model_name}_{dataset_name}.json"
                if eval_file.exists():
                    with open(eval_file, 'r') as f:
                        eval_data = json.load(f)
                    if eval_data:
                        eval_results = eval_data[0] if isinstance(eval_data, list) else eval_data
                        
            else:
                print(f"âš ï¸  Ã‰valuation standard Ã©chouÃ©e: {result.stderr[:200]}")
                eval_results = {
                    'map50': 0.0,
                    'map50_95': 0.0,
                    'precision': 0.0,
                    'recall': 0.0,
                    'f1_score': 0.0
                }
                
            # MÃ©triques supplÃ©mentaires
            print("ğŸ“ Calcul des mÃ©triques supplÃ©mentaires...")
            
            # Taille du modÃ¨le
            total_params, model_size_mb = self.metrics_calculator.calculate_model_size(weights_path)
            
            # FPS
            fps = self.metrics_calculator.calculate_fps(weights_path, dataset_yaml, self.device)
            
            # Combiner toutes les mÃ©triques
            comprehensive_results = {
                **eval_results,
                'total_parameters': total_params,
                'model_size_mb': round(model_size_mb, 2),
                'fps': round(fps, 2),
                'weights_path': weights_path,
                'evaluation_status': 'success'
            }
            
            print(f"ğŸ“Š MÃ©triques: mAP@0.5={comprehensive_results.get('map50', 0):.3f}, "
                  f"FPS={comprehensive_results.get('fps', 0):.1f}, "
                  f"Params={comprehensive_results.get('total_parameters', 0):,}")
                  
            return comprehensive_results
            
        except Exception as e:
            print(f"ğŸ’¥ Erreur Ã©valuation complÃ¨te: {e}")
            return {
                'evaluation_status': 'error',
                'error': str(e),
                'map50': 0.0,
                'map50_95': 0.0,
                'fps': 0.0,
                'total_parameters': 0,
                'model_size_mb': 0.0
            }
            
    def run_comprehensive_benchmark(self, 
                                  models: Optional[List[str]] = None, 
                                  datasets: Optional[List[str]] = None,
                                  validate_results: bool = True) -> Dict[str, Any]:
        """Lance le benchmark mÃ©thodologique complet."""
        
        # ModÃ¨les Ã  tester
        if models is None:
            models = self.model_factory.get_all_models()
        
        # Datasets Ã  tester
        if datasets is None:
            datasets = list(self.datasets.keys())
        
        print(f"\nğŸ¯ BENCHMARK MÃ‰THODOLOGIQUE COMPLET")
        print(f"ğŸ“¦ ModÃ¨les: {len(models)} ({', '.join(models[:3])}{'...' if len(models) > 3 else ''})")
        print(f"ğŸ“Š Datasets: {len(datasets)} ({', '.join(datasets)})")
        print(f"âš™ï¸  Protocole: epochs={self.epochs}, batch={self.batch_size}, img_size={self.img_size}")
        print(f"ğŸ–¥ï¸  Device: {self.device}")
        print(f"ğŸ” Validation des rÃ©sultats: {validate_results}")
        
        # RÃ©sultats globaux
        all_results = []
        total_experiments = len(models) * len(datasets)
        current_experiment = 0
        successful_experiments = 0
        
        for dataset_name in datasets:
            dataset_yaml = self.datasets[dataset_name]
            
            for model_name in models:
                current_experiment += 1
                print(f"\n{'='*80}")
                print(f"ğŸ§ª ExpÃ©rience {current_experiment}/{total_experiments}: {model_name} Ã— {dataset_name}")
                print(f"{'='*80}")
                
                # EntraÃ®nement avec protocole unifiÃ©
                train_result = self.train_model_with_unified_protocol(
                    model_name, dataset_name, dataset_yaml
                )
                
                # RÃ©sultat de base
                experiment_result = {
                    'experiment_id': f"{model_name}_{dataset_name}",
                    'model_name': model_name,
                    'dataset_name': dataset_name,
                    'dataset_yaml': dataset_yaml,
                    'training_status': train_result['status'],
                    'training_time': train_result.get('training_time', 0),
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'protocol_version': '1.0_comprehensive',
                    'model_info': train_result.get('model_info', {}),
                    'preprocessing_config': train_result.get('preprocessing_config', {})
                }
                
                # Ã‰valuation si l'entraÃ®nement a rÃ©ussi
                if train_result['status'] == 'success' and train_result.get('weights_path'):
                    eval_result = self.evaluate_model_comprehensive(
                        train_result['weights_path'], 
                        dataset_yaml, 
                        model_name, 
                        dataset_name
                    )
                    experiment_result.update(eval_result)
                    
                    if eval_result.get('evaluation_status') == 'success':
                        successful_experiments += 1
                        
                        # Validation des rÃ©sultats
                        if validate_results:
                            validation_result = self._validate_results(experiment_result)
                            experiment_result['validation'] = validation_result
                            
                else:
                    experiment_result.update({
                        'evaluation_status': 'not_evaluated',
                        'map50': 0.0,
                        'map50_95': 0.0,
                        'fps': 0.0,
                        'total_parameters': 0,
                        'model_size_mb': 0.0
                    })
                
                all_results.append(experiment_result)
                
                # Sauvegarde intermÃ©diaire
                self._save_intermediate_results(all_results)
                
                # Affichage du progrÃ¨s
                success_rate = (successful_experiments / current_experiment) * 100
                print(f"\nğŸ“ˆ ProgrÃ¨s: {current_experiment}/{total_experiments} "
                      f"({success_rate:.1f}% succÃ¨s)")
        
        print(f"\nğŸ‰ BENCHMARK COMPLET TERMINÃ‰!")
        print(f"ğŸ“Š {len(all_results)} expÃ©riences rÃ©alisÃ©es")
        print(f"âœ… {successful_experiments} expÃ©riences rÃ©ussies ({(successful_experiments/len(all_results)*100):.1f}%)")
        
        # GÃ©nÃ©rer le rapport final complet
        final_report = self._generate_comprehensive_report(all_results)
        
        return {
            'total_experiments': len(all_results),
            'successful_experiments': successful_experiments,
            'success_rate': (successful_experiments / len(all_results)) * 100,
            'results': all_results,
            'summary_file': str(self.results_dir / 'comprehensive_results.json'),
            'report_file': str(self.results_dir / 'comprehensive_report.csv'),
            'analysis_file': str(self.results_dir / 'methodology_analysis.json'),
            'final_report': final_report
        }
        
    def _validate_results(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Valide la cohÃ©rence et la logique des rÃ©sultats."""
        validation = {
            'is_valid': True,
            'warnings': [],
            'errors': []
        }
        
        # VÃ©rifications de cohÃ©rence
        map50 = result.get('map50', 0)
        map50_95 = result.get('map50_95', 0)
        fps = result.get('fps', 0)
        model_size = result.get('model_size_mb', 0)
        
        # mAP@0.5 doit Ãªtre >= mAP@0.5:0.95
        if map50_95 > map50:
            validation['errors'].append(f"mAP@0.5:0.95 ({map50_95:.3f}) > mAP@0.5 ({map50:.3f})")
            validation['is_valid'] = False
            
        # VÃ©rifications de plausibilitÃ©
        if map50 > 1.0:
            validation['errors'].append(f"mAP@0.5 impossible: {map50:.3f}")
            validation['is_valid'] = False
            
        if fps < 0 or fps > 1000:
            validation['warnings'].append(f"FPS suspect: {fps:.1f}")
            
        if model_size < 0 or model_size > 1000:
            validation['warnings'].append(f"Taille modÃ¨le suspecte: {model_size:.1f} MB")
            
        # RÃ©sultats trop bons pour Ãªtre vrais
        if map50 > 0.99:
            validation['warnings'].append(f"mAP@0.5 suspicieusement Ã©levÃ©: {map50:.3f}")
            
        return validation
        
    def _save_intermediate_results(self, results: List[Dict[str, Any]]):
        """Sauvegarde intermÃ©diaire des rÃ©sultats."""
        output_file = self.results_dir / 'comprehensive_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
    def _generate_comprehensive_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """GÃ©nÃ¨re un rapport complet avec analyses."""
        
        # DataFrame pour l'analyse
        df = pd.DataFrame(results)
        
        # Rapport CSV dÃ©taillÃ©
        columns_order = [
            'model_name', 'dataset_name', 'training_status', 'evaluation_status',
            'map50', 'map50_95', 'precision', 'recall', 'f1_score',
            'fps', 'total_parameters', 'model_size_mb', 'training_time',
            'experiment_id', 'timestamp'
        ]
        
        existing_cols = [col for col in columns_order if col in df.columns]
        df_report = df[existing_cols]
        
        # Sauvegarder le rapport CSV
        report_file = self.results_dir / 'comprehensive_report.csv'
        df_report.to_csv(report_file, index=False, encoding='utf-8')
        
        # Analyses statistiques
        successful = df[df['training_status'] == 'success']
        evaluated = df[df['evaluation_status'] == 'success']
        
        analysis = {
            'summary': {
                'total_experiments': len(df),
                'successful_training': len(successful),
                'successful_evaluation': len(evaluated),
                'training_success_rate': (len(successful) / len(df)) * 100 if len(df) > 0 else 0,
                'evaluation_success_rate': (len(evaluated) / len(df)) * 100 if len(df) > 0 else 0
            },
            'model_performance': {},
            'dataset_difficulty': {},
            'methodology_insights': {}
        }
        
        if len(evaluated) > 0:
            # VÃ©rifier quelles colonnes existent vraiment
            available_metrics = []
            for metric in ['map50', 'map50_95', 'fps', 'training_time', 'precision', 'recall']:
                if metric in evaluated.columns and not evaluated[metric].isna().all():
                    available_metrics.append(metric)
            
            if available_metrics:
                # Performance par modÃ¨le (seulement pour les mÃ©triques disponibles)
                agg_dict = {}
                for metric in available_metrics:
                    agg_dict[metric] = ['mean', 'std', 'count']
                
                try:
                    model_stats = evaluated.groupby('model_name').agg(agg_dict).round(3)
                    # Convertir en format JSON sÃ©rialisable
                    model_performance = {}
                    for model in model_stats.index:
                        model_performance[model] = {}
                        for metric in available_metrics:
                            model_performance[model][metric] = {
                                'mean': float(model_stats.loc[model, (metric, 'mean')]),
                                'std': float(model_stats.loc[model, (metric, 'std')]),
                                'count': int(model_stats.loc[model, (metric, 'count')])
                            }
                    analysis['model_performance'] = model_performance
                except Exception as e:
                    print(f"âš ï¸  Erreur calcul performance modÃ¨les: {e}")
                    analysis['model_performance'] = {}
                
                # DifficultÃ© par dataset
                try:
                    if 'map50' in available_metrics:
                        dataset_stats = evaluated.groupby('dataset_name').agg({
                            'map50': ['mean', 'std', 'count']
                        }).round(3)
                        # Convertir en format JSON sÃ©rialisable
                        dataset_difficulty = {}
                        for dataset in dataset_stats.index:
                            dataset_difficulty[dataset] = {
                                'map50_mean': float(dataset_stats.loc[dataset, ('map50', 'mean')]),
                                'map50_std': float(dataset_stats.loc[dataset, ('map50', 'std')]),
                                'count': int(dataset_stats.loc[dataset, ('map50', 'count')])
                            }
                        analysis['dataset_difficulty'] = dataset_difficulty
                except Exception as e:
                    print(f"âš ï¸  Erreur calcul difficultÃ© datasets: {e}")
                    analysis['dataset_difficulty'] = {}
                
                # Insights mÃ©thodologiques
                try:
                    insights = {}
                    
                    if 'map50' in evaluated.columns and not evaluated['map50'].isna().all():
                        best_idx = evaluated['map50'].idxmax()
                        insights['best_overall_model'] = evaluated.loc[best_idx]['model_name']
                        insights['average_map50'] = float(evaluated['map50'].mean())
                    
                    if 'fps' in evaluated.columns and not evaluated['fps'].isna().all():
                        fastest_idx = evaluated['fps'].idxmax()
                        insights['fastest_model'] = evaluated.loc[fastest_idx]['model_name']
                        insights['average_fps'] = float(evaluated['fps'].mean())
                    
                    if 'training_time' in evaluated.columns and not evaluated['training_time'].isna().all():
                        insights['total_training_time_hours'] = float(evaluated['training_time'].sum() / 3600)
                    
                    analysis['methodology_insights'] = insights
                    
                except Exception as e:
                    print(f"âš ï¸  Erreur calcul insights: {e}")
                    analysis['methodology_insights'] = {}
        
        # Sauvegarder l'analyse
        analysis_file = self.results_dir / 'methodology_analysis.json'
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # Affichage des rÃ©sultats principaux
        print(f"\nğŸ“Š ANALYSE DES RÃ‰SULTATS:")
        print(f"âœ… EntraÃ®nements rÃ©ussis: {analysis['summary']['successful_training']}/{analysis['summary']['total_experiments']} ({analysis['summary']['training_success_rate']:.1f}%)")
        print(f"ğŸ“ˆ Ã‰valuations rÃ©ussies: {analysis['summary']['successful_evaluation']}/{analysis['summary']['total_experiments']} ({analysis['summary']['evaluation_success_rate']:.1f}%)")
        
        if len(evaluated) > 0 and analysis['methodology_insights']:
            insights = analysis['methodology_insights']
            if 'best_overall_model' in insights:
                print(f"ğŸ† Meilleur modÃ¨le: {insights['best_overall_model']}")
            if 'fastest_model' in insights:
                print(f"âš¡ ModÃ¨le le plus rapide: {insights['fastest_model']}")
            if 'average_map50' in insights:
                print(f"ğŸ“Š mAP@0.5 moyen: {insights['average_map50']:.3f}")
            if 'total_training_time_hours' in insights:
                print(f"ğŸ•’ Temps total d'entraÃ®nement: {insights['total_training_time_hours']:.1f}h")
        
        print(f"ğŸ“ Rapport CSV: {report_file}")
        print(f"ğŸ“Š Analyse JSON: {analysis_file}")
        
        return analysis

def main():
    parser = argparse.ArgumentParser(description='Benchmark mÃ©thodologique complet')
    parser.add_argument('--epochs', type=int, default=50, 
                       help='Nombre d\'Ã©poques (dÃ©faut: 50)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Taille du batch (dÃ©faut: 16)')
    parser.add_argument('--img_size', type=int, default=640,
                       help='Taille des images (dÃ©faut: 640)')
    parser.add_argument('--device', default='auto',
                       help='Device (auto/cpu/cuda)')
    parser.add_argument('--models', nargs='*',
                       help='ModÃ¨les spÃ©cifiques Ã  tester')
    parser.add_argument('--datasets', nargs='*', 
                       help='Datasets spÃ©cifiques Ã  tester')
    parser.add_argument('--quick', action='store_true',
                       help='Test rapide (10 Ã©poques, YOLOv8n uniquement)')
    parser.add_argument('--no-augmentation', action='store_true',
                       help='DÃ©sactiver l\'augmentation de donnÃ©es')
    parser.add_argument('--no-validation', action='store_true',
                       help='DÃ©sactiver la validation des rÃ©sultats')
    
    args = parser.parse_args()
    
    # Mode test rapide
    if args.quick:
        args.epochs = 10
        args.models = ['yolov8n.pt']
        print("ğŸš€ Mode test rapide activÃ© (10 Ã©poques, YOLOv8n seulement)")
    
    # CrÃ©er le benchmark
    benchmark = ComprehensiveMethodologyBenchmark(
        device=args.device,
        epochs=args.epochs,
        batch_size=args.batch_size,
        img_size=args.img_size,
        augmentation=not args.no_augmentation
    )
    
    # Lancer le benchmark complet
    results = benchmark.run_comprehensive_benchmark(
        models=args.models,
        datasets=args.datasets,
        validate_results=not args.no_validation
    )
    
    print(f"\nğŸ¯ BENCHMARK TERMINÃ‰:")
    print(f"ğŸ“Š Taux de succÃ¨s: {results['success_rate']:.1f}%")
    print(f"ğŸ“ RÃ©sultats: {results['summary_file']}")
    print(f"ğŸ“Š Rapport: {results['report_file']}")
    print(f"ğŸ” Analyse: {results['analysis_file']}")

if __name__ == '__main__':
    main()