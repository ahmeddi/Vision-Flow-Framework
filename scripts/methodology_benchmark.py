#!/usr/bin/env python3
"""
Script de benchmark complet pour la mÃ©thodologie de l'article.
Lance l'entraÃ®nement et l'Ã©valuation de tous les modÃ¨les sur tous les datasets 
avec un protocole unifiÃ©.

ModÃ¨les testÃ©s:
- YOLOv8 (n, s, m, l, x)
- YOLOv11 (n, s, m, l, x) 
- YOLO-NAS (s, m, l)
- YOLOX (nano, tiny, s, m, l, x)
- YOLOv7 (tiny, s, m, l, x)
- PP-YOLOE (s, m, l, x)
- EfficientDet (d0-d7)
- DETR (base, large)
- RT-DETR (l, x)

Datasets:
- Weed25 (25 espÃ¨ces de mauvaises herbes)
- DeepWeeds (8 espÃ¨ces, environnements variÃ©s)
- CWD30 (20 espÃ¨ces + cultures)
- WeedsGalore (UAV multispectral, segmentation)

MÃ©triques collectÃ©es:
- mAP@0.5, mAP@0.5:0.95
- FPS, latence (P95)
- Taille du modÃ¨le (paramÃ¨tres, MB)
- Consommation Ã©nergÃ©tique
"""

import argparse
import json
import time
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd

class MethodologyBenchmark:
    """Gestionnaire de benchmark pour la mÃ©thodologie de l'article."""
    
    def __init__(self, device: str = 'auto', epochs: int = 100, batch_size: int = 16):
        self.device = device
        self.epochs = epochs
        self.batch_size = batch_size
        self.results_dir = Path('results/methodology_benchmark')
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Configuration des modÃ¨les par famille
        self.model_configs = {
            'yolov8': ['yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt'],
            'yolov11': ['yolov11n.pt', 'yolov11s.pt', 'yolov11m.pt', 'yolov11l.pt', 'yolov11x.pt'],
            'yolo_nas': ['yolo_nas_s', 'yolo_nas_m', 'yolo_nas_l'],
            'yolox': ['yolox_nano', 'yolox_tiny', 'yolox_s', 'yolox_m', 'yolox_l', 'yolox_x'],
            'yolov7': ['yolov7.pt'],
            'pp_yoloe': ['ppyoloe_s', 'ppyoloe_m', 'ppyoloe_l', 'ppyoloe_x'],
            'efficientdet': ['efficientdet_d0', 'efficientdet_d1', 'efficientdet_d2', 
                           'efficientdet_d3', 'efficientdet_d4'],
            'detr': ['detr_resnet50', 'detr_resnet101'],
            'rt_detr': ['rtdetr_l', 'rtdetr_x']
        }
        
        # Configuration des datasets
        self.datasets = {
            'weed25': 'data/weed25.yaml',
            'deepweeds': 'data/deepweeds.yaml', 
            'cwd30': 'data/cwd30.yaml',
            'weedsgalore': 'data/weedsgalore.yaml'
        }
        
        # VÃ©rifier les datasets disponibles
        self.available_datasets = self._check_datasets()
        
    def _check_datasets(self) -> Dict[str, str]:
        """VÃ©rifie quels datasets sont disponibles."""
        available = {}
        for name, yaml_path in self.datasets.items():
            if Path(yaml_path).exists():
                # VÃ©rifier si les donnÃ©es existent aussi
                data_dir = Path(f'data/{name}')
                if data_dir.exists() and any(data_dir.iterdir()):
                    available[name] = yaml_path
                else:
                    print(f"âš ï¸  Dataset {name}: YAML trouvÃ© mais donnÃ©es manquantes")
            else:
                print(f"âš ï¸  Dataset {name}: Configuration YAML manquante")
        
        if not available:
            print("âš ï¸  Aucun dataset disponible, utilisation du dataset dummy")
            available['dummy'] = 'data/dummy.yaml'
            
        return available
    
    def get_available_models(self) -> List[str]:
        """Retourne la liste des modÃ¨les disponibles selon les dÃ©pendances."""
        available_models = []
        
        # YOLOv8/v11 toujours disponibles (ultralytics)
        available_models.extend(self.model_configs['yolov8'])
        available_models.extend(self.model_configs['yolov11'])
        
        # VÃ©rifier les autres dÃ©pendances
        try:
            import super_gradients
            available_models.extend(self.model_configs['yolo_nas'])
        except ImportError:
            print("âš ï¸  YOLO-NAS non disponible (super-gradients manquant)")
            
        try:
            import yolox
            available_models.extend(self.model_configs['yolox'])
        except ImportError:
            print("âš ï¸  YOLOX non disponible (yolox manquant)")
            
        # PP-YOLOE, EfficientDet, DETR nÃ©cessitent des packages spÃ©ciaux
        # On les garde dans la liste mais ils Ã©choueront gracieusement
        
        return available_models
    
    def train_model(self, model_name: str, dataset_name: str, dataset_yaml: str) -> Dict[str, Any]:
        """EntraÃ®ne un modÃ¨le sur un dataset."""
        print(f"\nğŸš€ EntraÃ®nement: {model_name} sur {dataset_name}")
        
        # CrÃ©er le rÃ©pertoire de sortie
        output_dir = self.results_dir / f"{model_name}_{dataset_name}"
        
        # Commande d'entraÃ®nement
        cmd = [
            sys.executable, 'scripts/train.py',
            '--models', model_name,
            '--data', dataset_yaml,
            '--epochs', str(self.epochs),
            '--batch-size', str(self.batch_size),
            '--device', self.device,
            '--output', str(output_dir)
        ]
        
        try:
            start_time = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)  # 1h timeout
            training_time = time.time() - start_time
            
            if result.returncode == 0:
                print(f"âœ… EntraÃ®nement rÃ©ussi en {training_time:.1f}s")
                
                # Trouver le fichier de poids (chercher dans output_dir)
                # Le script train.py sauvegarde les modÃ¨les dans output_dir/model_name/
                model_dirs = list(output_dir.glob('**/'))
                best_weights = None
                
                for model_dir in model_dirs:
                    weights_dir = model_dir / 'weights'
                    if weights_dir.exists():
                        best_path = weights_dir / 'best.pt'
                        if best_path.exists():
                            best_weights = best_path
                            break
                
                return {
                    'status': 'success',
                    'training_time': training_time,
                    'weights_path': str(best_weights) if best_weights else None,
                    'output_dir': str(output_dir)
                }
            else:
                print(f"âŒ Ã‰chec de l'entraÃ®nement: {result.stderr}")
                return {
                    'status': 'failed',
                    'error': result.stderr,
                    'training_time': training_time
                }
                
        except subprocess.TimeoutExpired:
            print(f"â° Timeout de l'entraÃ®nement aprÃ¨s 1h")
            return {
                'status': 'timeout',
                'training_time': 3600
            }
        except Exception as e:
            print(f"ğŸ’¥ Erreur lors de l'entraÃ®nement: {e}")
            return {
                'status': 'error',
                'error': str(e),
                'training_time': 0
            }
    
    def evaluate_model(self, weights_path: str, dataset_yaml: str, model_name: str, dataset_name: str) -> Dict[str, Any]:
        """Ã‰value un modÃ¨le entraÃ®nÃ©."""
        print(f"ğŸ“Š Ã‰valuation: {model_name} sur {dataset_name}")
        
        cmd = [
            sys.executable, 'scripts/evaluate.py',
            '--weights', weights_path,
            '--data', dataset_yaml,
            '--device', self.device,
            '--output', str(self.results_dir / f"eval_{model_name}_{dataset_name}.json")
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f"âœ… Ã‰valuation rÃ©ussie")
                
                # Charger les rÃ©sultats
                eval_file = self.results_dir / f"eval_{model_name}_{dataset_name}.json"
                if eval_file.exists():
                    with open(eval_file, 'r') as f:
                        eval_results = json.load(f)
                    return eval_results[0] if eval_results else {}
                else:
                    return {'status': 'no_results'}
            else:
                print(f"âŒ Ã‰chec de l'Ã©valuation: {result.stderr}")
                return {'status': 'eval_failed', 'error': result.stderr}
                
        except Exception as e:
            print(f"ğŸ’¥ Erreur lors de l'Ã©valuation: {e}")
            return {'status': 'eval_error', 'error': str(e)}
    
    def run_full_benchmark(self, models: List[str] = None, datasets: List[str] = None) -> Dict[str, Any]:
        """Lance le benchmark complet."""
        
        # ModÃ¨les Ã  tester
        if models is None:
            models = self.get_available_models()
        
        # Datasets Ã  tester
        if datasets is None:
            datasets = list(self.available_datasets.keys())
        
        print(f"\nğŸ¯ BENCHMARK MÃ‰THODOLOGIE")
        print(f"ğŸ“¦ ModÃ¨les: {len(models)} ({', '.join(models[:5])}...)")
        print(f"ğŸ“Š Datasets: {len(datasets)} ({', '.join(datasets)})")
        print(f"âš™ï¸  ParamÃ¨tres: epochs={self.epochs}, batch_size={self.batch_size}")
        print(f"ğŸ–¥ï¸  Device: {self.device}")
        
        # RÃ©sultats globaux
        all_results = []
        total_experiments = len(models) * len(datasets)
        current_experiment = 0
        
        for dataset_name in datasets:
            dataset_yaml = self.available_datasets[dataset_name]
            
            for model_name in models:
                current_experiment += 1
                print(f"\n{'='*60}")
                print(f"ExpÃ©rience {current_experiment}/{total_experiments}: {model_name} Ã— {dataset_name}")
                print(f"{'='*60}")
                
                # EntraÃ®nement
                train_result = self.train_model(model_name, dataset_name, dataset_yaml)
                
                # RÃ©sultat de base
                experiment_result = {
                    'model_name': model_name,
                    'dataset_name': dataset_name,
                    'dataset_yaml': dataset_yaml,
                    'training_status': train_result['status'],
                    'training_time': train_result.get('training_time', 0),
                    'experiment_id': f"{model_name}_{dataset_name}",
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # Ã‰valuation si l'entraÃ®nement a rÃ©ussi
                if train_result['status'] == 'success' and train_result.get('weights_path'):
                    eval_result = self.evaluate_model(
                        train_result['weights_path'], 
                        dataset_yaml, 
                        model_name, 
                        dataset_name
                    )
                    experiment_result.update(eval_result)
                else:
                    experiment_result.update({
                        'map50': 0.0,
                        'map50_95': 0.0,
                        'fps': 0.0,
                        'total_parameters': 0,
                        'model_size_mb': 0.0
                    })
                
                all_results.append(experiment_result)
                
                # Sauvegarde intermÃ©diaire
                self._save_results(all_results)
        
        print(f"\nğŸ‰ BENCHMARK TERMINÃ‰!")
        print(f"ğŸ“ˆ {len(all_results)} expÃ©riences rÃ©alisÃ©es")
        
        # GÃ©nÃ©rer le rapport final
        self._generate_report(all_results)
        
        return {
            'total_experiments': len(all_results),
            'results': all_results,
            'summary_file': str(self.results_dir / 'methodology_results.json'),
            'report_file': str(self.results_dir / 'methodology_report.csv')
        }
    
    def _save_results(self, results: List[Dict[str, Any]]):
        """Sauvegarde les rÃ©sultats."""
        output_file = self.results_dir / 'methodology_results.json'
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
    
    def _generate_report(self, results: List[Dict[str, Any]]):
        """GÃ©nÃ¨re un rapport CSV pour l'analyse."""
        df = pd.DataFrame(results)
        
        # Colonnes principales pour l'article
        columns_order = [
            'model_name', 'dataset_name', 'training_status',
            'map50', 'map50_95', 'fps', 'total_parameters', 
            'model_size_mb', 'training_time'
        ]
        
        # RÃ©organiser les colonnes
        existing_cols = [col for col in columns_order if col in df.columns]
        df_report = df[existing_cols]
        
        # Sauvegarder
        report_file = self.results_dir / 'methodology_report.csv'
        df_report.to_csv(report_file, index=False)
        
        print(f"ğŸ“Š Rapport sauvegardÃ©: {report_file}")
        
        # Statistiques rapides
        successful = df[df['training_status'] == 'success']
        print(f"âœ… EntraÃ®nements rÃ©ussis: {len(successful)}/{len(df)}")
        
        if len(successful) > 0:
            print(f"ğŸ“ˆ mAP@0.5 moyen: {successful['map50'].mean():.3f}")
            print(f"âš¡ FPS moyen: {successful['fps'].mean():.1f}")

def main():
    parser = argparse.ArgumentParser(description='Benchmark complet pour mÃ©thodologie')
    parser.add_argument('--epochs', type=int, default=10, 
                       help='Nombre d\'Ã©poques pour l\'entraÃ®nement (dÃ©faut: 10)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Taille du batch (dÃ©faut: 16)')
    parser.add_argument('--device', default='auto',
                       help='Device pour l\'entraÃ®nement (auto/cpu/cuda)')
    parser.add_argument('--models', nargs='*',
                       help='ModÃ¨les spÃ©cifiques Ã  tester (dÃ©faut: tous)')
    parser.add_argument('--datasets', nargs='*', 
                       help='Datasets spÃ©cifiques Ã  tester (dÃ©faut: tous)')
    parser.add_argument('--quick', action='store_true',
                       help='Test rapide (5 Ã©poques, YOLOv8n uniquement)')
    
    args = parser.parse_args()
    
    # Mode test rapide
    if args.quick:
        args.epochs = 5
        args.models = ['yolov8n.pt']
        print("ğŸš€ Mode test rapide activÃ© (5 Ã©poques, YOLOv8n seulement)")
    
    # CrÃ©er le benchmark
    benchmark = MethodologyBenchmark(
        device=args.device,
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    
    # Lancer le benchmark
    results = benchmark.run_full_benchmark(
        models=args.models,
        datasets=args.datasets
    )
    
    print(f"\nğŸ¯ RÃ‰SULTATS FINAUX:")
    print(f"ğŸ“ RÃ©sultats: {results['summary_file']}")
    print(f"ğŸ“Š Rapport: {results['report_file']}")

if __name__ == '__main__':
    main()